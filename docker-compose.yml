services:
  db:
    image: postgres:15-alpine
    container_name: whatsappcrm_db
    volumes:
      - postgres_data:/var/lib/postgresql/data/
    environment:
      POSTGRES_DB: ${DB_NAME:-whatsapp_crm_dev}
      POSTGRES_USER: ${DB_USER:-crm_user}
      POSTGRES_PASSWORD: ${DB_PASSWORD:?Please set DB_PASSWORD in .env file}
    ports: # In production, you can comment this out for security if the DB is only accessed by other containers.
      - "5432:5432" # Consider using ${DB_PORT_LOCAL:-5432}:5432 for flexibility
    healthcheck: # Added healthcheck for robustness
      test: ["CMD-SHELL", "pg_isready -U $${POSTGRES_USER:-crm_user} -d $${POSTGRES_DB:-whatsapp_crm_dev}"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: whatsappcrm_redis
    volumes:
      - redis_data:/data
    ports: # In production, you can comment this out for security if Redis is only accessed by other containers.
      - "6379:6379" # Consider using ${REDIS_PORT_LOCAL:-6379}:6379
    healthcheck: # Added healthcheck
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  backend:
    build: ./whatsappcrm_backend
    container_name: whatsappcrm_backend_app
    # command: implicitly uses CMD from Dockerfile (which should use entrypoint.sh web)
    volumes: # For production, it's best practice to remove the code bind mount ('./whatsappcrm_backend:/app').
      - ./whatsappcrm_backend:/app
      - staticfiles_volume:/app/staticfiles # Ensure this matches STATIC_ROOT
      - media_volume:/app/mediafiles      # Ensure this matches MEDIA_ROOT
    ports: # Added port mapping for direct access if Nginx is not always used or for dev
      - "${DJANGO_PORT_LOCAL:-8000}:8000"
    env_file:
      - ./whatsappcrm_backend/.env
    environment: # Ensure Django settings can find services
      - DJANGO_SETTINGS_MODULE=whatsappcrm_backend.settings
      - PYTHONUNBUFFERED=1
      - DB_HOST=db
      # CRITICAL FIX: The line below is removed. It was overriding the password-protected URL from the .env file.
      # - CELERY_BROKER_URL=redis://redis:6379/0
    depends_on:
      db:
        condition: service_healthy # Wait for DB to be healthy
      redis:
        condition: service_healthy # Wait for Redis to be healthy
    restart: unless-stopped

  frontend:
    build: ./whatsapp-crm-frontend
    container_name: whatsappcrm_frontend_app
    # command: (For production, this should serve static files, not run a dev server)
    # ports: (e.g. "3000:3000" if dev server runs on 3000)
    volumes:
      - ./whatsapp-crm-frontend:/app
      - /app/node_modules # Persist node_modules
    depends_on:
      - backend # Or nginx_proxy if it serves the frontend
    restart: unless-stopped

  celery_worker:
    build: ./whatsappcrm_backend
    container_name: whatsappcrm_celery_worker
    # MODIFIED command to explicitly set pool and concurrency via env vars
    command: >
      sh -c "
        echo 'Celery Worker: Waiting for DB (db:5432)...' &&
        while ! nc -z db 5432; do sleep 1; done;
        echo 'Celery Worker: DB is up.' &&
        echo 'Celery Worker: Waiting for Redis (redis:6379)...' &&
        while ! nc -z redis 6379; do sleep 1; done;
        echo 'Celery Worker: Redis is up.' &&
        echo 'Starting Celery worker with Pool: $${CELERY_WORKER_POOL_TYPE:-gevent} and Concurrency: $${CELERY_WORKER_CONCURRENCY:-100}' &&
        celery -A whatsappcrm_backend.celery worker -l INFO \
          -P $${CELERY_WORKER_POOL_TYPE:-gevent} \
          -c $${CELERY_WORKER_CONCURRENCY:-100} \
          --without-gossip --without-mingle --without-heartbeat -E
      "
    volumes:
      - ./whatsappcrm_backend:/app
    env_file:
      - ./whatsappcrm_backend/.env # For Django app settings used by Celery tasks
    environment:
      # These are for the command above and for Celery app running inside container.
      # Define these in a root .env file (next to docker-compose.yml) or set defaults here.
      - CELERY_WORKER_POOL_TYPE=${CELERY_WORKER_POOL_TYPE:-gevent} # Example: gevent or prefork
      - CELERY_WORKER_CONCURRENCY=${CELERY_WORKER_CONCURRENCY:-100} # This is read from .env
      # Ensure Celery can find Django settings and connect to services
      - DJANGO_SETTINGS_MODULE=whatsappcrm_backend.settings
      - PYTHONUNBUFFERED=1
      - DB_HOST=db
      # CRITICAL FIX: The line below is removed to use the password-protected URL from the .env file.
      # - CELERY_BROKER_URL=redis://redis:6379/0
    depends_on: # Updated for robustness
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
      backend: # Ensures backend (which might run migrations) starts
        condition: service_started
    restart: unless-stopped

  celery_beat:
    build: ./whatsappcrm_backend
    container_name: whatsappcrm_celery_beat
    command: celery -A whatsappcrm_backend beat -l INFO --scheduler django_celery_beat.schedulers:DatabaseScheduler # Original command
    volumes:
      - ./whatsappcrm_backend:/app
    env_file:
      - ./whatsappcrm_backend/.env
    environment: # Added for consistency and explicitness
      - DJANGO_SETTINGS_MODULE=whatsappcrm_backend.settings
      - PYTHONUNBUFFERED=1
      - DB_HOST=db
      # CRITICAL FIX: The line below is removed to use the password-protected URL from the .env file.
      # - CELERY_BROKER_URL=redis://redis:6379/0
    depends_on: # Updated for robustness
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
      backend:
        condition: service_started
    restart: unless-stopped

  nginx_proxy:
    image: nginx:1.25-alpine
    container_name: whatsappcrm_nginx_proxy
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx_proxy/nginx.conf:/etc/nginx/conf.d/default.conf:ro
      - staticfiles_volume:/srv/www/static/:ro
      - media_volume:/srv/www/media/:ro
      - /var/www/letsencrypt:/var/www/letsencrypt:ro
      - /etc/letsencrypt:/etc/nginx/ssl:ro
    depends_on:
      - backend
      - frontend
    restart: unless-stopped
    healthcheck: # Added healthcheck
      test: ["CMD-SHELL", "wget -q --spider --proxy=off http://localhost/ || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3

volumes:
  postgres_data:
  redis_data:
  staticfiles_volume:
  media_volume: # This volume is shared between 'backend' and 'nginx_proxy'
